{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens inside your CNN: conv2d\n",
    "### (discrete 2-D convolutions)\n",
    "This short gist assumes some basic knowledge about fully-connected neural networks - the kind everyone is first introduced to. Here we try to understand in detail what happens when using a 2-D convolutional layer in our neural networks. Convolutional layers and CNNs in general are very popular in domains like Computer Vision, but can also be applied to many other types of data with an intrinsic structure (e.g. sound clips or text).  \n",
    "\n",
    "### Why?\n",
    "One might ask why should we even bother with convolutions in the first place?  \n",
    "In short: With discrete convolutions we can exploit the *intrinsic structure*  of the given data (if present). We'll use image inputs as an example. Their intrinsic structure is obviously in two dimensions, therefore 2-D convolutions are useful in this case. Bonus: They are also the easiest to visualize.  \n",
    "Another reason to use convolutions is due to the fact that images possess *localized concepts*. This means pixels close to each other share some correlations. They are likely to be similar, unless there's an edge at that location - which also is an important feature we don't want to miss. The same goes e.g. for text where words next to each other probably share the same context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do discrete convolutions work?\n",
    "Discrete convolutions can basically be described as \"a linear transformations which preserve ordering\" ([from this nice paper on which a lot of this notebook is based](https://arxiv.org/pdf/1603.07285.pdf)).  \n",
    "This is achieved by a **kernel** of a given size (here 2-dimensional *height x width*) sliding across the **input feature map** (here one color channel of the image). At each **stride** the product between each element of the kernel and the input element it overlaps is computed and all elements are summed up to obtain the output of the current stride. This process is repeated stride by stride to produce **output feature maps**.  \n",
    "Note that for a 3-D convolution the kernel would simply be a cuboid sliding across height, width and depth. The 1-D case is now rather obvious. But the  effect for all cases is the same: The convolution preserves the intrinsic structure of the input.\n",
    "<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\">\n",
    "Here the blue map represents the input and the cyan map the output feature map.  \n",
    "Any kernel size other than `(1, 1)` will results in smaller output feature maps compared to the input. As can be seen above the kernel size of `(3, 3)` turns the `(4, 4)` input feature map into a `(2, 2)` output. If we want the input and output feature maps to be of the same size we can simply *pad* the input with zeros.  \n",
    "For example a `(5, 5)` input feature map with a **zero-padding** of 1 pixel for each dimension preserves the spatial size of the input even with the kernel size of `(3, 3)`.\n",
    "<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif\">\n",
    "The reasons why we may want to preserve the spatial size of the input are manifold:\n",
    "* designing networks is easier since the tensor dimensions will simply fit\n",
    "* allows for deeper networks (without padding the size will be reduced too quickly)\n",
    "* can improve performance by keeping information at the borders\n",
    "* some newer architectures need to concatenate convolutional layers with  \n",
    "`(1, 1)`, `(3, 3)` and `(5, 5)` kernels, which wouldn't be possible without padding since the dimensions wouldn't match (see [inception module](https://i.stack.imgur.com/ldTdM.png) for example)  \n",
    "\n",
    "We can also choose to have the kernel move across the input with larger steps if we use a larger **stride**. As can be seen in the example below with a kernel size of `(3, 3)` and a stride of `(2, 2)`, larger strides quickly reduce the spatial size of the input and result in less computations per convolution compared to smaller strides, but also leads to information loss. So there's a trade-off.\n",
    "<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are the weights in convolutional layers?\n",
    "When comparing convolutional layers to their 'classical' counterpart of fully-connected layers, instead of having weight matrices we now have kernels sliding across the input. The question may arise: Where are the trainable weights in this case?  \n",
    "For convolutional layers the weights are contained in the collection of kernels we use. Biases aren't treated any different from the way they are used in fully-connected layers.  \n",
    "So if `(5, 5)` kernel contains 25 only weights that really aren't too many. After all, a big advantage of convolutional layers is that weights are applied to multiple locations in the input and therefore they get by with a much smaller number of weights.  \n",
    "But usually we do not noly have one kernel, but rather many more depending on how many output feature maps we wish to create and how many input feature maps we have at hand. The choice of the number of output feature maps of a convolutional layer can be compared to the choice of the number of hidden nodes in a fully-connected layer.  \n",
    "To see how this all comes together let's take a look at the formula given in the [PyTorch docs](http://pytorch.org/docs/master/nn.html#conv2d) on `torch.nn.Conv2d`:\n",
    "$$ \\text{out}(N_i, C_{\\text{out}, j}) = \\text{bias}(C_{\\text{out}, j}) \n",
    "+ \\sum_{k=0}^{C_{\\text{in}-1}} \\text{weight}(C_{\\text{out}, j}, k) \\star \\text{input}(N_i, k)$$\n",
    "This formula maps our input tensor [$N, C_\\text{in}, H, W$] to our output tensor [$N, C_\\text{out}, H_\\text{out}, W_\\text{out}$]. The $\\star$ operator can be seen as the operation of sliding the kernel across the input feature map (recall, as described above, there's another summation hidden in this operation). $N$ simply describes the number of inputs (also known as batch size) to take into account for the optimization step. Since this is not central to understanding convolutional layers N can be put aside for this consideration. $C$ denotes the respective feature map, $\\text{weight}$ is our kernel and $\\text{bias}$ should be self-explanatory.  \n",
    "From this we can quickly infer that we have $C_\\text{out} \\cdot C_\\text{in}$ kernels.  \n",
    "\n",
    "Let's look at an example of a convolutional layer used e.g. in a network for classifying handwritten digits  on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerConv2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneLayerConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, \n",
    "                               stride=1, padding=0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toy network now only consists of one `nn.Conv2d` layer with *1 input feature map* and *10 output feature maps*. There's no pooling layer or activation function as there would be in the case of an actual convolutional neural used for training.  \n",
    "We can print a summay of our network to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneLayerConv2d(\n",
      "  (conv1): Conv2d (1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "conv_layer = OneLayerConv2d()\n",
    "print(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch let's use access all weights (here called *parameters*) of our network. The `.parameters()` method returns a generator, so let's look at the resulting list. Weights and biases are separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -> parameters for weights and biases\n"
     ]
    }
   ],
   "source": [
    "params = list(conv_layer.parameters())\n",
    "print(len(params), \"-> parameters for weights and biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check our expectations for the total number of parameters of our convolutional layer. With our 1 input feature map, 10 output feature maps, kernel size of `(5, 5)` and stride of `(1, 1)` we'd expect 10 kernels with 25 elements each. That makes for a total of 260 parameters including the 10 bias elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n",
      "Number of parameters (from kernels):\t 250\n",
      "Number of parameters (from biases):\t 10\n"
     ]
    }
   ],
   "source": [
    "print(params[0].size())\n",
    "print(\"Number of parameters (from kernels):\\t\",\n",
    "     reduce(lambda x, y: x*y, params[0].size()))\n",
    "print(\"Number of parameters (from biases):\\t\", params[1].size()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the whole kernel tensor to get a better feeling for the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor containing all kernel weights:\t torch.Size([10, 1, 5, 5])\n",
      "\n",
      " Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.1965  0.0802 -0.1461  0.0131 -0.1233\n",
      " -0.0910  0.0738 -0.1316 -0.0076  0.0710\n",
      " -0.1218  0.1586  0.0425 -0.1391 -0.0013\n",
      "  0.1792  0.0705 -0.0948 -0.0328  0.0845\n",
      " -0.0739  0.0163  0.0941 -0.1632 -0.0426\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0847 -0.0284  0.1238  0.1300 -0.1828\n",
      "  0.0823  0.1470 -0.0860  0.1757  0.0675\n",
      " -0.0816  0.1099  0.1546 -0.0650 -0.1576\n",
      " -0.0984  0.0536 -0.0089 -0.0841 -0.0489\n",
      "  0.1224  0.0897 -0.1336 -0.1076  0.0706\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.0662 -0.0020 -0.1265 -0.0934 -0.1166\n",
      " -0.1951 -0.0393  0.1113 -0.1773  0.0455\n",
      "  0.1373 -0.0544  0.1666 -0.1128  0.1679\n",
      "  0.1410  0.1341  0.0930  0.0549 -0.0343\n",
      "  0.0623 -0.0167 -0.1185  0.0468 -0.1171\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.0734 -0.0134  0.1997 -0.0012  0.0735\n",
      " -0.1987 -0.0150  0.0486 -0.1723 -0.0884\n",
      " -0.0332 -0.1228  0.0138 -0.1539  0.1722\n",
      "  0.1726  0.0068 -0.1813 -0.0672 -0.1922\n",
      "  0.1976 -0.0994 -0.1151  0.0470  0.0705\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.0590  0.0809  0.0980  0.1608 -0.1570\n",
      "  0.1033  0.0966 -0.1464 -0.1254 -0.1196\n",
      "  0.0298  0.0569  0.1506 -0.1335  0.0863\n",
      "  0.0789 -0.0615 -0.1634 -0.0240 -0.0455\n",
      "  0.0541 -0.1488  0.1345 -0.1566  0.1060\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0612 -0.1640 -0.0611  0.1001 -0.0059\n",
      "  0.0224 -0.0755 -0.0821 -0.0328  0.0499\n",
      " -0.1991 -0.1719  0.1575  0.1175  0.1579\n",
      " -0.0358 -0.1932  0.0842 -0.1893  0.1529\n",
      "  0.1803 -0.0261  0.0529 -0.0279  0.0144\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.1549 -0.0859 -0.0080  0.0911  0.0399\n",
      "  0.1622  0.1584 -0.1574  0.0305  0.0622\n",
      " -0.0363  0.1663 -0.0923  0.0523 -0.1631\n",
      "  0.1778 -0.0303  0.1526 -0.0330  0.0481\n",
      "  0.1621 -0.1307 -0.1013 -0.0663 -0.1163\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1099 -0.0938 -0.0560  0.1802  0.0910\n",
      "  0.1431 -0.0067 -0.0126 -0.1975 -0.0367\n",
      " -0.1486  0.0675 -0.0059  0.1071  0.0186\n",
      " -0.1480  0.0271 -0.1019 -0.1632 -0.0353\n",
      " -0.1204  0.0420  0.1832 -0.1713 -0.0664\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0369  0.1895 -0.0318 -0.1403  0.1585\n",
      "  0.1503  0.1682 -0.1641 -0.1500  0.1866\n",
      " -0.0988 -0.1218  0.0359  0.0316 -0.1480\n",
      "  0.0806  0.1942  0.1017  0.1370  0.1896\n",
      " -0.0129  0.1008 -0.0908 -0.1724  0.1630\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "  0.0445 -0.0410 -0.1957  0.0670  0.0092\n",
      " -0.0632 -0.0642  0.1653 -0.1201  0.1102\n",
      "  0.1578 -0.0563  0.1044 -0.1973  0.0059\n",
      " -0.0391  0.0524 -0.0230 -0.0848  0.0150\n",
      "  0.1541  0.0160  0.0123 -0.0400 -0.0277\n",
      "[torch.FloatTensor of size 10x1x5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor containing all kernel weights:\\t\", list(conv_layer.parameters())[0].size())\n",
    "print(\"\\n\",list(conv_layer.parameters())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this convolutional layer to some dummy input. We'll use the dimensions which would be typical for training a neural network on MNIST data. A batch size of 128 samples with 28x28 grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t\t\t torch.Size([128, 1, 28, 28])\n",
      "After Conv2d:\t\t torch.Size([128, 10, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.rand(128, 1, 28, 28))\n",
    "output = conv_layer(x)\n",
    "print('Input:\\t\\t\\t', x.shape)\n",
    "print('After Conv2d:\\t\\t', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'd expect sliding a `(5, 5)` kernel across a `(28, 28)` input feature map with a stride of `(1, 1)` leaves us with `(24, 24)` output feature maps if we don't use zero-padding.\n",
    "\n",
    "That concludes this 'discrete 2-D convolutions behind-the-scenes'. There are more options to `conv2d` like dilation or groups which were omitted here for the sake of brevity. For details on these, the reader is referred again to the [PyTorch docs](http://pytorch.org/docs/master/nn.html#conv2d) on `torch.nn.Conv2d` and the following two papers:  \n",
    "*A guide to convolution arithmetic for deep\n",
    "learning* ([*arXiv:1603.07285*](https://arxiv.org/pdf/1603.07285.pdf))  \n",
    "*Multi-scale context aggregation by dilated convolutions* ([*arXiv:1511.07122*](https://arxiv.org/pdf/1511.07122.pdf))\n",
    "\n",
    "The next step for taking a look behind the scenes of convolutional neural networks is to explore **pooling layers**, which will be covered in the next part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
