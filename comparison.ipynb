{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "batch_size = 128\n",
    "batch_size_test = 1000\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "use_cuda = True\n",
    "rnd_seed = 1\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CapsNet\n",
    "from capsnet import CapsNetWithReconstruction, CapsNet, ReconstructionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Pad(2), transforms.RandomCrop(28),\n",
    "                           transforms.ToTensor()\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t\t\t torch.Size([128, 1, 28, 28])\n",
      "After Conv2d:\t\t torch.Size([128, 10, 24, 24])\n",
      "After MaxPool2d:\t torch.Size([128, 10, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "    out = conv1(data)\n",
    "    out1 = F.max_pool2d(out,2)\n",
    "    print('Input:\\t\\t\\t', data.shape)\n",
    "    print('After Conv2d:\\t\\t', out.shape)\n",
    "    print('After MaxPool2d:\\t', out1.shape)\n",
    "    #if batch_idx>0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: One 2-D convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerConv2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneLayerConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                               kernel_size=5, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneLayerConv2d(\n",
      "  (conv1): Conv2d (1, 1, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "convnet = OneLayerConv2d()\n",
    "print(convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -> parameters for weights and biases\n",
      "torch.Size([1, 1, 5, 5])\n",
      "Number of trainable parameters:\t 25\n"
     ]
    }
   ],
   "source": [
    "params = list(convnet.parameters())\n",
    "print(len(params), \"-> parameters for weights and biases\")\n",
    "print(params[0].size())\n",
    "print(\"Number of trainable parameters:\\t\",\n",
    "     reduce(lambda x, y: x*y, params[0].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0625  0.1986  0.0105 -0.0787  0.0376\n",
      "  0.1810 -0.0522  0.1730 -0.0166  0.1566\n",
      "  0.1139  0.0135  0.1453  0.1922  0.0805\n",
      "  0.1817 -0.0617 -0.0859  0.0328 -0.1083\n",
      "  0.0135 -0.1607  0.0380  0.0685  0.1088\n",
      "[torch.FloatTensor of size 1x1x5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list(convnet.parameters())[0].size())\n",
    "print(list(convnet.parameters())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: One fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerFullyConnected(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneLayerFullyConnected, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=576, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneLayerFullyConnected(\n",
      "  (fc1): Linear(in_features=784, out_features=576)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fcnet = OneLayerFullyConnected()\n",
    "print(fcnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -> parameters for weights and biases\n",
      "torch.Size([576, 784])\n",
      "Number of trainable parameters:\t 451584\n"
     ]
    }
   ],
   "source": [
    "params = list(fcnet.parameters())\n",
    "print(len(params), \"-> parameters for weights and biases\")\n",
    "print(params[0].size())\n",
    "print(\"Number of trainable parameters:\\t\",\n",
    "     reduce(lambda x, y: x*y, params[0].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([576, 784])\n",
      "Parameter containing:\n",
      " 2.1294e-02 -1.1966e-02 -5.5920e-03  ...  -2.3773e-02  1.6400e-02 -3.2261e-02\n",
      " 3.5528e-02 -3.4197e-02  1.7607e-02  ...  -1.1237e-02  1.4912e-02 -8.1181e-03\n",
      " 3.5313e-04 -2.7233e-02 -1.2361e-02  ...  -2.1450e-02 -1.2769e-02 -3.4277e-02\n",
      "                ...                   â‹±                   ...                \n",
      "-1.1810e-02  1.8567e-02 -1.0469e-02  ...  -2.3108e-02  3.8369e-03  1.4605e-02\n",
      " 1.2541e-02 -9.4283e-03  3.3070e-02  ...   2.4976e-03 -1.6350e-02  4.7672e-03\n",
      "-2.6617e-02  1.8238e-02 -2.1221e-03  ...   1.3060e-02 -2.4435e-02  3.3660e-03\n",
      "[torch.FloatTensor of size 576x784]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list(fcnet.parameters())[0].size())\n",
    "print(list(fcnet.parameters())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
